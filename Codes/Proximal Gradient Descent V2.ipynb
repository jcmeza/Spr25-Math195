{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "abfbdc2f-dbd8-4be1-b328-d774a4e305b5",
      "cell_type": "markdown",
      "source": "# V2",
      "metadata": {}
    },
    {
      "id": "d9b678d4-6079-4598-a2ad-5162fac4aca0",
      "cell_type": "markdown",
      "source": "## Libraries Used",
      "metadata": {}
    },
    {
      "id": "b7961cec-5fd1-4ec2-bb14-b9dde06d2244",
      "cell_type": "code",
      "source": "import numpy as np\nfrom scipy.optimize import minimize_scalar, minimize \nimport time\nimport matplotlib.pyplot as plt\nimport pandas as pd",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Matplotlib is building the font cache; this may take a moment.\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "3ae49167-5f40-4d2a-99b4-867712c0f56e",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "478d5feb-40d1-4638-bcdb-67652bc3d519",
      "cell_type": "markdown",
      "source": "## Exact Line Search\n- Computes the optimal step size along the gradient at each iteration ",
      "metadata": {}
    },
    {
      "id": "d6e16868-d96a-47fd-8c6c-9697d47cfcd5",
      "cell_type": "code",
      "source": "# finding optimal alpha (step-size)\n# g: function g(x), x: current point, direction: gradient at point x\n\n# def exact_line_search(g, x, direction):\ndef exact_line_search(g, x, direction, A, b):\n    # This is how we can write a function in one line\n    # phi = lambda alpha: g(x + alpha * direction)\n    \n    def phi(alpha):\n        return g(x - alpha*direction, A, b)\n    \n    # solves local minimization of scalar function of one variable\n    # Runs our phi function and tests various alpha values (input) \n    # until it finds one that minimizes the function\n    res = minimize_scalar(phi) # returns an OptimizeResult object\n\n    return res.x # The value of alpha that minimizes phi",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "503db57a-52c7-4f6c-9f5f-cf6875be7e84",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "e54ff18c-b5a3-4218-8e2c-f19427417f9b",
      "cell_type": "markdown",
      "source": "## Proximal Operators of L1 and L2 Euclidean Norms",
      "metadata": {}
    },
    {
      "id": "2a75af97-7ecc-4803-bdc6-e1be3b8a80f7",
      "cell_type": "code",
      "source": "# implements l1 norm of lambda * ||x||\ndef prox_l1_norm(x, alpha_lamb):\n    # prox operator = max(|x| - λα, 0) * sign(x)\n    return np.maximum(np.abs(x) - alpha_lamb, 0) * np.sign(x)\n\n# implements l2 norm of lambda * ||x||_2\ndef prox_l2_norm(x, lamb):\n\n    # prox operator = (1- λ/(max(||x||, λ))) * x\n    \n    normed = np.linalg.norm(x, 2)\n    den = np.maximum(normed, lamb)\n    return (1 - lamb/den) * x",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "c5ad9026-79b4-40f0-8857-db323ec5aca9",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "b774fa2e-5100-4f8d-b2f7-ad4aafd4284e",
      "cell_type": "markdown",
      "source": "## Subgradient of $\\|Ax - b\\|_1$\n\n- The subgradient is given by: $A^T \\cdot s$,  \n  where $A^T$ is the transpose of $A$, and $s$ is the sign vector of $(Ax - b)$.",
      "metadata": {}
    },
    {
      "id": "86c683e8-2b44-4c7c-8e70-2031af7f7f9b",
      "cell_type": "code",
      "source": "def sub_grad_g(x, A, b, tol=1e-6):\n\n    # say r = Ax-b\n    r = A @ x - b             # shape (m,)\n    # r = (A @ x - b).flatten()\n    \n    # sign matrix intialization (for indexing)\n    s = np.zeros_like(r)      # shape (m,)\n\n    # for every row or entry in the vector\n    for i in range(len(r)):\n        # if positive val, sign = 1\n        if r[i] > tol:\n            s[i] = 1.0\n        # if negative val, sign = -1\n        elif r[i] < -tol:\n            s[i] = -1.0\n        else:\n            # r[i] is close to 0, pick any subgradient in [-1, 1].\n            # We'll pick 0 for simplicity.\n            s[i] = 0.0\n    \n    # Now the subgradient w.r.t x is A^T @ s\n    g_sub = A.T @ s  # shape (n,)\n    # g_sub = A.T @ np.sign(r)  \n    return g_sub",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "894f3671-30b3-4a1f-9529-0f20c34cc382",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "ca80609e-3f7e-4247-b856-1f259dbd0074",
      "cell_type": "markdown",
      "source": "## Functions: $g(x) = \\|Ax - b\\|_1$ and $h(x) = \\|x\\|_1$\n\n- Where:\n  - Matrix $A \\in \\mathbb{R}^{m \\times n}$  \n  - Vector $b \\in \\mathbb{R}^{m}$ \n  - Vector $x \\in \\mathbb{R}^{n}$\n    - $n$ being the number of columns in $A$",
      "metadata": {}
    },
    {
      "id": "24a22a08-5aa2-4230-b24b-cce51b957503",
      "cell_type": "code",
      "source": "# A * x results in a product matrix of mx1, the same dimensions of b,\n# thus, you can now substract the two\n\ndef g(x, A, b):\n    # @ is used for matrix multiplication\n    return np.linalg.norm(A @ x - b, ord=1)\n\ndef h(x):\n    # Convert x to a 1D array before computing the norm by using flatten()\n    # NumPy treats a column vector differently from a 1D array. \n        # If x is a 2D array with shape (n, 1), np.linalg.norm(x, ord=1) will fail.\n    return np.linalg.norm(x.flatten(), ord=1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "302a17f6-e915-4692-a981-539d86579f0b",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "7aa54359-0fe0-4778-b51a-b0f00be88196",
      "cell_type": "markdown",
      "source": "## Proximal Gradient Descent Method",
      "metadata": {}
    },
    {
      "id": "86a3f28e-4085-432a-b1f6-e0fec2cf761a",
      "cell_type": "code",
      "source": "# added A and b\n# def proximal_descent(g, g_prime, h, h_prox, x0, A, b, lamb, iterations = 20000, alpha = 1e-6, tol = 1e-4):\ndef proximal_descent(g, g_prime, h, h_prox, x0, A, b, lamb, alpha = 1, iterations = 200, tol = 1e-4):\n    \"\"\"\n    minimizes a non-differentiable function f(x) = g(x) + h(x)\n    PARAMS\n    g: function\n        g(x), the differentiable part of f\n    \n    g_prime: function\n        g'(x) aka the gradient of g\n        returns the direction of steepest increase along g\n\n    h: function\n        h(x), the non-differentiable part of f\n        \n    h_prox: function\n        h_prox(x, alpha) returns proximal operator of h at x using alpha as a distance weighting param\n        h_prox gives a new x' which is a tradeoff of reducing h and staying close to x\n        \n    x0: vector\n        initial starting point\n\n    A: mxn matrix\n\n    b: mx1 vector\n\n    lamb: lambda value used in g + λ*h\n\n    alpha: step size\n        \n    iterations: self explanitory\n    \n    tol: self explanitory\n    \n    RETURNS\n    x* = argmin_x { f(x) } if x* is reachable in the given num iterations along with following relevant data\n    \"\"\"\n    # initialize current guess at x0\n    xk = x0\n\n    func_values = [] # tracks the function values at each iteration\n    x_differences = [] # tracks the norm of the step size\n    func_differences = [] # tracks the difference of function values between xk (current) and xk_old\n    gradient_norms = [] # tracks the norm of the gradient at each iteration\n    h_norms = [] # tracks the norm of h at each iteration\n    alphas = [] # tracks the computed optimal alpha at each iteration\n\n    # for k iterations\n    for k in range(1, iterations+1):\n        # store old xk value\n        xk_old = xk\n\n        # compute gradient for differentiable part of f\n        gk_gradient = g_prime(xk, A, b)\n        \n        # keep track of the norm of the gradient in every iteration\n        gradient_norms.append(np.linalg.norm(gk_gradient))\n\n        # find optimal step size along the gradient using an exact line search\n        alpha_k = exact_line_search(g, xk, gk_gradient, A, b)\n        \n        # keep track of the optimal computed alpha in every iteration\n        alphas.append(alpha_k)\n        \n        # take gradient step to reduce g(x)\n        gradient_step = xk - alpha_k * gk_gradient  \n        \n        # find new x point \n        xk = h_prox(gradient_step, alpha_k*lamb)\n\n        # compute the function value at the current xk value\n        func_val_xk = g(xk, A, b) + lamb * h(xk)\n\n        # keep track of each computed function value in each iteration\n        func_values.append(func_val_xk)\n\n        # compute the function value at the old xk value\n        func_val_xold = g(xk_old, A, b) + lamb * h(xk_old)\n\n        # Compute the difference of function values between xk (current) and xk_old\n        func_differences.append(abs(func_val_xk - func_val_xold))\n        \n        # keep track of the norm of h evalauted at xk\n            # Helps analyze if the function is decreasing in every iteration (minimizing)\n        h_norms.append(np.linalg.norm(h(xk)))\n\n        # keep track of the norm of the step size\n        x_differences.append(np.linalg.norm(xk - xk_old, ord=2))\n\n        # Check if the norm of the step size is under our accepted tolerance\n        if np.linalg.norm(xk - xk_old, ord=2) < tol:\n            # If so we say it converged\n            print(\"Converged!\")\n\n            # return the following data\n            return xk, k, func_values, x_differences, func_differences, gradient_norms, alphas, h_norms\n\n    # Did not converge if the norm of the step size was not under our tolerance \n    print(\"Did not converge!\")\n\n    # return the following data\n    return xk, k, func_values, x_differences, func_differences, gradient_norms, alphas, h_norms",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "6299f56b-bb80-40a3-8192-777dbf7ba362",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "1b18cc8b-61e5-47ed-9985-435af9f7a44a",
      "cell_type": "markdown",
      "source": "## `SetUpProblem()`\n##### Initializes the true $x$ vector, matrix $A$, and vector $b$.\n- **Parameters:**\n  - $m$: number of rows\n  - $n$: number of columns\n  - $lamb$: lambda\n  - *case_num*: Test problem number\n   \n- **Returns**\n  - Matrix $A \\in \\mathbb{R}^{m \\times n}$\n  - Vector $b \\in \\mathbb{R}^{m}$\n  - True solution vector $x \\in \\mathbb{R}^{n}$  ",
      "metadata": {}
    },
    {
      "id": "d1396572-1168-4357-ae0a-571a287c6b76",
      "cell_type": "code",
      "source": "def SetUpProblem(m, n, lamb, case_num):\n    match case_num:\n        case 1:\n            # Assume the true solution of x to be a vector of all 1's\n            x_true = np.ones((n, 1))\n\n            # random mxn matrix\n            A = np.random.rand(m, n) \n\n            # force vector be to be equal to Ax\n                # Cancels out the first term g(x)\n            b = A @ x_true \n\n            return A, b, x_true\n        case 2:\n            # Assume the true solution of x to be a vector of all 0's except let the first entry be 1\n            x_true = np.zeros((n, 1))\n            x_true[0,0] = 1\n\n            # random mxn matrix\n            A = np.random.rand(m, n) \n\n            # force vector be to be equal to Ax\n                # Cancels out the first term g(x)\n            b = A @ x_true \n\n            return A, b, x_true\n        case _:\n            return \"Case number not found or not passed\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "332ceadc-1fb5-4f0a-b9c4-f258b12ab2e7",
      "cell_type": "markdown",
      "source": "## `run_problem()`\n\n- The function runs the **Proximal Gradient Method** using the following parameters passed in.\n  - Evaluates the algorithm using an initial $x$ vector of all 0's\n- **Returns**\n  - **final_x**\n    - Final $x$ vector that the Proximal Gradient Descent algorithm returned (regardless of convergence or not)\n  - **itera**\n    - Number of iterations it took to converge/not converge (max iterations)\n  - **func_values**\n    - List of function values computed at current x-vector at each iteration\n  - **x_diffs**\n    - List of the norm of the step size at each iteration\n  - **func_diffs**\n    - List of difference in function values evaluated at xk (current) and xk_old (x-vector from the previous iteration)\n  - **grads**\n    - List of the norm of the gradient at each iteration\n  - **returned_alphas**\n    - List of optimized alphas computed at each iteration\n  - **h_norms**\n    - List of the norm of h computed at the current x-vector (xk) at each iteration\n  - **t_total**\n    - Time spent inside the Proximal Gradient Descent algorithm\n  - **init_func_val**\n    - Initial function value evaluated at x_0\n  - **final_func_val**\n    - Final function value evaluated at final_x ",
      "metadata": {}
    },
    {
      "id": "15118475-a09c-48ce-b564-41115045c51d",
      "cell_type": "code",
      "source": "def run_problem(g, sub_grad_g, h, prox_oper, lamb, init_alpha, A, b):\n\n    \"\"\"\n    g: g(x) function that has a computable sub-gradient\n    \n    sub_grad_g: The subgradient of g(x)\n    \n    h: The function h(x) that is not necessarily differentiable \n    \n    prox_oper: Function computing the proximal operator of an L1 norm\n    \n    lamb: Lambda value\n    \n    init_alpha: Initial step size to use for the Proximal Gradient Descent Method\n    \n    A: mxn matrix\n\n    b: mx1 matrix\n        \n    \"\"\"\n    \n    # Set the initial vector x_0 to be all zeros\n    x_0 = np.zeros((A.shape[0], 1)) # nx1 vector of all 0's\n\n    # Compute function value at x_0\n    init_func_val = g(x_0, A, b) + lamb*h(x_0)\n\n    # Start the timer\n    t0 = time.time() \n\n    # call the Proximal Gradient Descent Method\n    final_x, itera, func_values, x_diffs, func_diffs, grads, returned_alphas, h_norms = proximal_descent(g, sub_grad_g, h, prox_oper, x_0, A, b, lamb, init_alpha)\n\n    # End the timer\n    tf = time.time() \n\n    # Compute the total elpased time\n    t_total = tf-t0 \n\n    # Compute final function value at final_x\n    final_func_val = g(final_x, A, b) + lamb*h(final_x)\n\n    return final_x, itera, func_values, x_diffs, func_diffs, grads, returned_alphas, h_norms, t_total, init_func_val, final_func_val    ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "1cddbf60-b8d0-4d2f-bc97-bb47573ff890",
      "cell_type": "markdown",
      "source": "## Helper function to build a dataframe",
      "metadata": {}
    },
    {
      "id": "a4703a48-98d1-4f76-836e-9fe78417825d",
      "cell_type": "code",
      "source": "def build_data_frame(func_values, func_diffs, x_diffs, returned_alphas, grads, h_norms):\n    \"\"\"\n    Parameters are the values returned from proximal_descent\n    \n    func_values: \n        List of function values computed at current x-vector at each iteration\n    \n    func_diffs: \n        List of difference in function values evaluated at xk (current) and \n        xk_old (x-vector from the previous iteration)\n    \n    x_diffs: \n        List of the norm of the step size at each iteration\n    \n    returned_alphas: \n        List of optimized alphas computed at each iteration\n    \n    grads: \n        List of the norm of the gradient at each iteration\n    \n    h_norms: \n        List of the norm of h computed at the current x-vector (xk) at each iteration\n    \n    \"\"\"\n    \n    # object to help create a pandas dataframe (table)\n    data = {} \n    \n    # data = {key: column_name, val: values to display}\n    data['Function Value'] = func_values \n    data['Func diff'] = func_diffs\n    data['||Step Size||'] = x_diffs\n    data['alphas'] = returned_alphas\n    data['||gradient||'] = grads\n    data['h_norms'] = h_norms\n\n    # create the table using the created object\n    df = pd.DataFrame(data) \n\n    return df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "46193553-116d-4c90-881e-c689682a52ad",
      "cell_type": "markdown",
      "source": "## Helper function to print out:\n- Matrix dimensions of $A$\n- Initial function value: $f(x_0)$\n- Final function value: $f(x_{final})$\n  - **Note**: $x_{final}$ may not be the true $x$ that minimizes $\\|Ax-b\\|_1 + 2\\|x\\|_1$\n    - e.g. non-convergence still returns some $x_{final}$\n- Elapsed time spent in the **Proximal Gradient Descent Method**\n- Absolute error between $\\| x_{true} \\|$ and $\\| x_{final} \\|$",
      "metadata": {}
    },
    {
      "id": "c0daf27e-ff0f-4e5b-bd74-1f5b17b123c0",
      "cell_type": "code",
      "source": "def print_proximal_gd_metrics(m, n, init_func_val, final_func_val, time_elasped, final_x, x_true):\n    \"\"\"\n\n    m,n: Matrix Dimensions of A\n\n    init_func_val: Initial function value evaluated at x_0\n\n    final_func_val: \n        Final function value evaluated at the last last x-vector that \n        the Proximal Gradient Descent algorithm returned\n\n    time_elapsed: Time spent inside the Proximal Gradient Descent algorithm\n\n    final_x: the last last x-vector that the Proximal Gradient Descent algorithm returned\n\n    x_true: x-vector that we chose to which we know the expected solution of f(x_true)\n    \n    \"\"\"\n    \n    print(f\"Matrix Dimensions of A: {m}x{n}\")\n    print(\"--------------\"*4)\n    \n    print(f\"Initial Function Value f(x_0): {init_func_val}\")\n\n    print(f'Final Function Value f(x_final): {final_func_val}')\n\n    print(f\"Elapsed Time taken: {t_total}s\")\n\n    print(f\"abs( ||x*|| - ||x|| ): {abs(np.linalg.norm(x_true) - np.linalg.norm(final_x))}\")\n    print()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "d3a33dc0-1a01-4928-a0e7-ce1fdc23c839",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    },
    {
      "id": "92a6a839-e472-454f-a31f-71091a4279e8",
      "cell_type": "markdown",
      "source": "## Test Problem 1\n\n- $min \\{ \\|Ax-b\\|_1 + \\lambda \\|x\\|_1 \\}$\n  - $A$ be an $m \\times n$ matrix\n  - $x$ be an $n \\times 1$ vector\n  - $b$ be an $m \\times 1$ vector  \n  - $\\lambda = 2$\n\n- Assume the true solution of $x$ to be a vector of all 1's\n    - Then, we expect $f_{min} = \\lambda \\|x\\|_1 = 2n$, if $Ax=b$",
      "metadata": {}
    },
    {
      "id": "6f821156-fd50-421d-bd22-10951e769dc7",
      "cell_type": "code",
      "source": "# define a list of dictionaries that represent all the dimensions \n# that you want to test the algorithm on\ndimensions = [{'m':5, 'n':5}, {'m':10, 'n':10}, {'m':20, 'n':20}, {'m':40, 'n':40}]\n\ntest_problem_num = 1\n\n# Lambda value to use\nlamb = 2\n\n# Initial step size (alpha) to use\ninit_alpha = 1\n\n# Intitialize matrix A, and vector's b and x_true based on the case number\nfor d in dimensions:\n    A, b, x_true = SetUpProblem(d['m'], d['n'], lamb, test_problem_num)\n    final_x, itera, func_values, x_diffs, func_diffs, grads, returned_alphas, h_norms, t_total, init_func_val, final_func_val = run_problem(g, sub_grad_g, h, prox_l1_norm, lamb, init_alpha, A, b)\n    print_proximal_gd_metrics(d['m'], d['n'], init_func_val, final_func_val, t_total, final_x, x_true)\n    df = build_data_frame(func_values, func_diffs, x_diffs, returned_alphas, grads, h_norms)\n    print(df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Did not converge!\nMatrix Dimensions of A: 5x5\n--------------------------------------------------------\nInitial Function Value f(x_0): 14.267014958264063\nFinal Function Value f(x_final): 9.229497653058015\nElapsed Time taken: 0.7320001125335693s\nabs( ||x*|| - ||x|| ): 0.14317380359242682\n\n     Function Value  Func diff  ||Step Size||    alphas  ||gradient||  \\\n0         12.480825   1.786190       0.778179  0.339025      6.506664   \n1         11.269700   1.211125       0.527644  0.229876      6.506664   \n2         10.448498   0.821203       0.357769  0.155867      6.506664   \n3          9.891682   0.556816       0.242585  0.105686      6.506664   \n4          9.729664   0.162018       0.164484  0.071660      6.506664   \n..              ...        ...            ...       ...           ...   \n195        9.237296   0.019303       0.027497  0.039171      4.732540   \n196        9.219614   0.017682       0.025189  0.035882      4.732540   \n197        9.227808   0.008193       0.023074  0.032869      4.732540   \n198        9.249767   0.021959       0.070200  0.049436      2.882941   \n199        9.229498   0.020269       0.028874  0.041131      4.732540   \n\n      h_norms  \n0    1.446624  \n1    2.427506  \n2    3.092593  \n3    3.543554  \n4    3.849328  \n..        ...  \n195  3.958792  \n196  3.989500  \n197  4.017630  \n198  3.909643  \n199  3.944844  \n\n[200 rows x 6 columns]\nDid not converge!\nMatrix Dimensions of A: 10x10\n--------------------------------------------------------\nInitial Function Value f(x_0): 51.480879387598144\nFinal Function Value f(x_final): 19.75384779775004\nElapsed Time taken: 0.6030001640319824s\nabs( ||x*|| - ||x|| ): 0.0014444454848230315\n\n     Function Value  Func diff  ||Step Size||    alphas  ||gradient||  \\\n0         31.756916  19.723963       1.944397  0.191680     16.395860   \n1         24.195555   7.561361       0.745402  0.073482     16.395860   \n2         21.296838   2.898717       0.285757  0.028170     16.395860   \n3         20.610118   0.686720       0.109547  0.010799     16.395860   \n4         20.536540   0.073578       0.026406  0.009476      7.689690   \n..              ...        ...            ...       ...           ...   \n195       19.767481   0.052268       0.006963  0.000928     13.619592   \n196       19.753311   0.014170       0.003251  0.000433     13.619592   \n197       19.756347   0.003036       0.001837  0.000668      4.707313   \n198       19.751372   0.004975       0.001738  0.000414     10.382198   \n199       19.753848   0.002476       0.001755  0.000638      4.707313   \n\n      h_norms  \n0    6.034241  \n1    8.347522  \n2    9.234340  \n3    9.574309  \n4    9.601239  \n..        ...  \n195  9.758507  \n196  9.768160  \n197  9.763917  \n198  9.769102  \n199  9.765049  \n\n[200 rows x 6 columns]\nConverged!\nMatrix Dimensions of A: 20x20\n--------------------------------------------------------\nInitial Function Value f(x_0): 199.41952425770882\nFinal Function Value f(x_final): 40.177306121780035\nElapsed Time taken: 0.1510000228881836s\nabs( ||x*|| - ||x|| ): 0.04862000683075607\n\n    Function Value   Func diff  ||Step Size||    alphas  ||gradient||  \\\n0        70.669764  128.749760       3.554265  0.098119     45.053937   \n1        45.404159   25.265604       0.697482  0.019255     45.053937   \n2        41.928395    3.475764       0.136873  0.003779     45.053937   \n3        41.658183    0.270213       0.022450  0.001865     20.254077   \n4        41.579427    0.078755       0.010118  0.000841     20.254077   \n5        41.880685    0.301257       0.068546  0.008504      7.611836   \n6        41.314554    0.566131       0.036365  0.001605     31.482638   \n7        41.266308    0.048245       0.008739  0.000726     20.254077   \n8        41.276612    0.010303       0.016729  0.002455      7.099149   \n9        41.184670    0.091942       0.010998  0.000718     23.923441   \n10       41.233348    0.048677       0.018934  0.002779      7.099149   \n11       41.100551    0.132796       0.011857  0.000774     23.923441   \n12       41.088377    0.012174       0.003170  0.000786      9.627285   \n13       41.083671    0.004706       0.003246  0.000525     13.995680   \n14       41.117270    0.033598       0.015498  0.002588      6.951223   \n15       41.032017    0.085252       0.008058  0.000762     19.033243   \n16       41.022620    0.009397       0.004025  0.000380     19.033243   \n17       41.015659    0.006961       0.001727  0.000429      9.627285   \n18       41.007818    0.007841       0.001945  0.000483      9.627285   \n19       40.998987    0.008832       0.002191  0.000544      9.627285   \n20       40.989039    0.009948       0.002468  0.000612      9.627285   \n21       40.977834    0.011205       0.002780  0.000690      9.627285   \n22       40.965213    0.012621       0.003131  0.000777      9.627285   \n23       40.950997    0.014216       0.003527  0.000875      9.627285   \n24       40.934985    0.016012       0.003973  0.000986      9.627285   \n25       40.916949    0.018036       0.004475  0.001110      9.627285   \n26       40.899377    0.017572       0.005040  0.001251      9.627285   \n27       40.891920    0.007456       0.005521  0.000893     13.995680   \n28       53.114230   12.222309       0.405535  0.040529      4.815961   \n29       42.164877   10.949352       0.302268  0.008344     45.053937   \n30       40.496009    1.668869       0.058631  0.001619     45.053937   \n31       40.394675    0.101334       0.009790  0.000560     26.289943   \n32       40.393701    0.000974       0.007578  0.001417      8.493804   \n33       40.367664    0.026037       0.004568  0.000346     21.883380   \n34       41.382395    1.014731       0.057571  0.006795      5.839293   \n35       40.332591    1.049804       0.041751  0.001315     40.583674   \n36       40.306671    0.025920       0.006617  0.000635     18.808641   \n37       40.284327    0.022343       0.004483  0.000900      9.650079   \n38       40.264869    0.019458       0.004237  0.000850      9.650079   \n39       40.258500    0.006369       0.002352  0.000380     13.995680   \n40       40.300722    0.042222       0.008607  0.001233      5.981714   \n41       40.232850    0.067872       0.005880  0.000336     26.289943   \n42       40.229159    0.003691       0.001357  0.000219     13.995680   \n43       40.254643    0.025484       0.006085  0.000872      5.981714   \n44       40.213503    0.041140       0.003861  0.000221     26.289943   \n45       40.209754    0.003749       0.000891  0.000144     13.995680   \n46       40.203947    0.005807       0.001441  0.000357      9.627285   \n47       40.198186    0.005761       0.001429  0.000355      9.627285   \n48       40.193293    0.004893       0.001418  0.000352      9.627285   \n49       40.191809    0.001485       0.000593  0.000069     17.048827   \n50       40.187139    0.004669       0.001159  0.000287      9.627285   \n51       40.183119    0.004021       0.001149  0.000285      9.627285   \n52       40.181984    0.001135       0.000226  0.000045     12.688427   \n53       40.181579    0.000405       0.000162  0.000032     12.688427   \n54       40.178107    0.003473       0.001029  0.000255      9.627285   \n55       40.177575    0.000532       0.000106  0.000021     12.688427   \n56       40.177306    0.000269       0.000076  0.000015     12.688427   \n\n      h_norms  \n0   15.642091  \n1   18.711664  \n2   19.314032  \n3   19.400470  \n4   19.439426  \n5   19.254403  \n6   19.414094  \n7   19.447741  \n8   19.401076  \n9   19.447370  \n10  19.394554  \n11  19.444463  \n12  19.443763  \n13  19.453961  \n14  19.410261  \n15  19.442689  \n16  19.458888  \n17  19.458507  \n18  19.458077  \n19  19.457593  \n20  19.457049  \n21  19.456435  \n22  19.455743  \n23  19.454964  \n24  19.454087  \n25  19.453099  \n26  19.451986  \n27  19.469332  \n28  17.879298  \n29  19.209559  \n30  19.467591  \n31  19.510313  \n32  19.497398  \n33  19.516816  \n34  19.316894  \n35  19.500769  \n36  19.526992  \n37  19.524358  \n38  19.521869  \n39  19.529256  \n40  19.500604  \n41  19.526262  \n42  19.530523  \n43  19.510269  \n44  19.527119  \n45  19.529918  \n46  19.529600  \n47  19.529284  \n48  19.528971  \n49  19.531338  \n50  19.531082  \n51  19.530829  \n52  19.531456  \n53  19.531906  \n54  19.531679  \n55  19.531973  \n56  19.532184  \nConverged!\nMatrix Dimensions of A: 40x40\n--------------------------------------------------------\nInitial Function Value f(x_0): 809.8168584867724\nFinal Function Value f(x_final): 81.516354325123\nElapsed Time taken: 0.1679999828338623s\nabs( ||x*|| - ||x|| ): 0.02285492885638085\n\n    Function Value   Func diff  ||Step Size||    alphas  ||gradient||  \\\n0       150.928383  658.888476       5.682947  0.049016    128.536605   \n1        87.436375   63.492008       0.557107  0.004805    128.536605   \n2        84.979331    2.457044       0.057303  0.000800     84.203259   \n3        84.703270    0.276061       0.030843  0.002245     14.748043   \n4        84.564244    0.139026       0.015222  0.000685     33.794022   \n..             ...         ...            ...       ...           ...   \n56       81.520237    0.001235       0.000127  0.000012     19.259910   \n57       81.519106    0.001131       0.000139  0.000014     14.095045   \n58       81.518119    0.000988       0.000105  0.000010     19.259910   \n59       81.517136    0.000983       0.000116  0.000012     14.095045   \n60       81.516354    0.000781       0.000088  0.000008     19.259910   \n\n      h_norms  \n0   35.772498  \n1   39.279327  \n2   39.639211  \n3   39.565542  \n4   39.649155  \n..        ...  \n56  39.781927  \n57  39.781725  \n58  39.781989  \n59  39.781821  \n60  39.782041  \n\n[61 rows x 6 columns]\n"
        }
      ],
      "execution_count": 11
    },
    {
      "id": "e93d55c6-9741-4f39-8a6f-00e3e6dcfb81",
      "cell_type": "markdown",
      "source": "## Test Problem 2\n\n- $min \\{ \\|Ax-b\\|_1 + 2\\|x\\|_1 \\}$\n  - $A$ be an $m \\times n$ matrix\n  - $x$ be an $n \\times 1$ vector \n  - $b$ be an $m \\times 1$ vector  \n  - $\\lambda = 2$\n\n- Assume the true solution of $x$ to be a vector of all 0's except let the first entry be 1\n  - Then, we expect $f_{min} = \\lambda \\|x\\|_1 = 2$, if $Ax=b$",
      "metadata": {}
    },
    {
      "id": "b18993d6-e33a-4504-8982-e964cc985265",
      "cell_type": "code",
      "source": "# define a list of dictionaries that represent all the dimensions \n# that you want to test the algorithm on\ndimensions = [{'m':5, 'n':5}, {'m':10, 'n':10}, {'m':20, 'n':20}, {'m':40, 'n':40}]\n\ntest_problem_num = 2\n\n# Lambda value to use\nlamb = 2\n\n# Initial step size (alpha) to use\ninit_alpha = 1\n\n# Intitialize matrix A, and vector's b and x_true based on the case number\nfor d in dimensions:\n    A, b, x_true = SetUpProblem(d['m'], d['n'], lamb, test_problem_num)\n    final_x, itera, func_values, x_diffs, func_diffs, grads, returned_alphas, h_norms, t_total, init_func_val, final_func_val = run_problem(g, sub_grad_g, h, prox_l1_norm, lamb, init_alpha, A, b)\n    print_proximal_gd_metrics(d['m'], d['n'], init_func_val, final_func_val, t_total, final_x, x_true)\n    df = build_data_frame(func_values, func_diffs, x_diffs, returned_alphas, grads, h_norms)\n    print(df)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Did not converge!\nMatrix Dimensions of A: 5x5\n--------------------------------------------------------\nInitial Function Value f(x_0): 3.1250919076888666\nFinal Function Value f(x_final): 2.0121095122553867\nElapsed Time taken: 0.4439997673034668s\nabs( ||x*|| - ||x|| ): 0.024062566016034737\n\n     Function Value  Func diff  ||Step Size||    alphas  ||gradient||  \\\n0          2.821959   0.303133       0.164532  0.089304      6.037234   \n1          2.609666   0.212293       0.115227  0.062542      6.037234   \n2          2.460992   0.148675       0.080697  0.043800      6.037234   \n3          2.435111   0.025881       0.056514  0.030674      6.037234   \n4          2.381204   0.053907       0.036736  0.037873      3.503554   \n..              ...        ...            ...       ...           ...   \n195        2.012811   0.000311       0.000665  0.001422      3.503554   \n196        2.012519   0.000292       0.000670  0.001431      3.503554   \n197        2.013103   0.000584       0.001500  0.000814      6.037234   \n198        2.012497   0.000607       0.000699  0.001057      3.503554   \n199        2.012110   0.000387       0.000622  0.001193      3.503554   \n\n      h_norms  \n0    0.294070  \n1    0.500016  \n2    0.644246  \n3    0.745254  \n4    0.696325  \n..        ...  \n195  0.989164  \n196  0.988995  \n197  0.991676  \n198  0.990969  \n199  0.990553  \n\n[200 rows x 6 columns]\nDid not converge!\nMatrix Dimensions of A: 10x10\n--------------------------------------------------------\nInitial Function Value f(x_0): 5.091849345971949\nFinal Function Value f(x_final): 2.1090576227020725\nElapsed Time taken: 0.5850000381469727s\nabs( ||x*|| - ||x|| ): 0.04931992444116051\n\n     Function Value  Func diff  ||Step Size||    alphas  ||gradient||  \\\n0          4.560192   0.531658       0.242155  0.026814     15.238813   \n1          4.076704   0.483487       0.116786  0.029848      6.854037   \n2          3.764796   0.311908       0.090904  0.027197      6.854037   \n3          3.542721   0.222075       0.072194  0.023469      6.854037   \n4          3.365545   0.177177       0.057598  0.018724      6.854037   \n..              ...        ...            ...       ...           ...   \n195        2.097419   0.000229       0.000141  0.000088      4.849559   \n196        2.097197   0.000222       0.000138  0.000085      4.849559   \n197        2.097094   0.000103       0.000103  0.000062      5.465595   \n198        2.140798   0.043704       0.022972  0.008674      3.435654   \n199        2.109058   0.031741       0.007459  0.001460     10.886909   \n\n      h_norms  \n0    0.741811  \n1    0.685189  \n2    0.711322  \n3    0.764555  \n4    0.807026  \n..        ...  \n195  1.000961  \n196  1.000978  \n197  1.001054  \n198  0.964342  \n199  0.980788  \n\n[200 rows x 6 columns]\nConverged!\nMatrix Dimensions of A: 20x20\n--------------------------------------------------------\nInitial Function Value f(x_0): 10.048132612011754\nFinal Function Value f(x_final): 3.644291346399078\nElapsed Time taken: 0.10699987411499023s\nabs( ||x*|| - ||x|| ): 0.35438389694027705\n\n    Function Value  Func diff  ||Step Size||    alphas  ||gradient||   h_norms\n0         7.497821   2.550311       0.136059  0.003761     45.036744  0.601623\n1         7.371460   0.126361       0.076602  0.006926     18.786350  0.862391\n2         7.146622   0.224839       0.254143  0.047203      8.649583  0.367276\n3         5.806837   1.339785       0.084309  0.002661     40.518389  0.738576\n4         5.708538   0.098298       0.047962  0.004336     18.786350  0.901846\n5         5.412594   0.295944       0.161911  0.030708      8.649583  0.604408\n6         4.781195   0.631399       0.060149  0.002174     36.498757  0.865234\n7         4.495357   0.285838       0.038206  0.005420     11.240541  0.858921\n8         4.327000   0.168357       0.027460  0.004566     11.240541  0.885193\n9         4.205141   0.121859       0.020937  0.003597     11.240541  0.912776\n10        4.116572   0.088570       0.015235  0.002640     11.240541  0.933771\n11        4.054236   0.062336       0.010950  0.001931     11.240541  0.951501\n12        4.013318   0.040917       0.007876  0.001391     11.240541  0.964747\n13        3.922471   0.090847       0.023852  0.004130      8.833291  0.946226\n14        3.882385   0.040085       0.007018  0.001229     11.240541  0.957388\n15        3.854281   0.028104       0.004945  0.000875     11.240541  0.965827\n16        3.834365   0.019916       0.003528  0.000625     11.240541  0.972049\n17        3.820103   0.014262       0.002527  0.000448     11.240541  0.976505\n18        3.809868   0.010235       0.001813  0.000321     11.240541  0.979703\n19        3.802307   0.007562       0.001340  0.000237     11.240541  0.982065\n20        3.796922   0.005385       0.000990  0.000175     11.240541  0.983811\n21        3.752862   0.044060       0.013351  0.002448      8.284379  0.971899\n22        3.730948   0.021914       0.003843  0.000674     11.240541  0.978251\n23        3.716300   0.014648       0.002569  0.000451     11.240541  0.982497\n24        3.706509   0.009791       0.001717  0.000301     11.240541  0.985336\n25        3.699963   0.006545       0.001148  0.000201     11.240541  0.987233\n26        3.695588   0.004375       0.000767  0.000135     11.240541  0.988501\n27        3.692664   0.002925       0.000513  0.000090     11.240541  0.989349\n28        3.691246   0.001418       0.000343  0.000060     11.240541  0.989916\n29        3.655871   0.035374       0.008390  0.001753      8.159369  0.986177\n30        3.652031   0.003841       0.000682  0.000121     11.240541  0.987422\n31        3.649342   0.002689       0.000477  0.000085     11.240541  0.988294\n32        3.647460   0.001882       0.000334  0.000059     11.240541  0.988905\n33        3.646142   0.001318       0.000234  0.000042     11.240541  0.989332\n34        3.645219   0.000923       0.000164  0.000029     11.240541  0.989631\n35        3.644573   0.000646       0.000115  0.000020     11.240541  0.989841\n36        3.644291   0.000282       0.000080  0.000014     11.240541  0.989987\nConverged!\nMatrix Dimensions of A: 40x40\n--------------------------------------------------------\nInitial Function Value f(x_0): 17.249733309852186\nFinal Function Value f(x_final): 3.0929230189719883\nElapsed Time taken: 0.29399967193603516s\nabs( ||x*|| - ||x|| ): 0.12830363153001867\n\n    Function Value  Func diff  ||Step Size||    alphas  ||gradient||   h_norms\n0        11.995788   5.253945       0.106513  0.000927    127.429065  0.669150\n1        11.700132   0.295656       0.023660  0.001434     24.612033  0.731447\n2         8.380769   3.319363       0.377788  0.040883     15.108316  0.762220\n3         7.581302   0.799468       0.030592  0.000640     58.477516  0.911607\n4         7.497409   0.083893       0.005195  0.000259     29.716984  0.932505\n..             ...        ...            ...       ...           ...       ...\n93        3.114877   0.007281       0.001957  0.000210     13.178112  1.115498\n94        3.096989   0.017888       0.000683  0.000023     38.517767  1.117464\n95        3.095159   0.001830       0.000137  0.000010     21.025018  1.117716\n96        3.093318   0.001842       0.000164  0.000015     15.373591  1.117572\n97        3.092923   0.000395       0.000039  0.000003     17.914384  1.117606\n\n[98 rows x 6 columns]\n"
        }
      ],
      "execution_count": 12
    },
    {
      "id": "8796c954-c4f7-4e2d-a958-9c2f9c176b29",
      "cell_type": "markdown",
      "source": "---",
      "metadata": {}
    }
  ]
}